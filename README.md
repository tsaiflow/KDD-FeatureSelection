# KDD-FeatureSelection

The explaination.pdf explains briefly the Problem formulation, Review of past techniques and the Solution part of this project.
Presentation file includes the pdf and ppt version of the presentation. Please read.
Pdf of code can be found in code_pdf, to view the results of my project. Non-PCA, PCA and borrowedFeature are 3 sets of data, their compared results can be found in detail in presenetation file.
Code can be found and inserted in code_ipynb file. 

Citations:
 Part of the code, mainly the normalizing and one-hot coding part was borrowed from the tutorial here (https://github.com/jeffheaton/t81_558_deep_learning/blob/master/tf_kdd99.ipynb). While we also borrowed the selected feature result from table 4 of the paper https://pdfs.semanticscholar.org/60e2/8c7da56eb61dd8ddb710a6f079ef02668014.pdf, the goal of this is to compare the accuracy with the PCA feature reduction approach. 

Dataset is not included in this git, should go to http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html to download. Need to download: 
  kddcup.data_10_percent.gz A 10% subset. (2.1M; 75M Uncompressed)
  kddcup.data.gz The full data set (18M; 743M Uncompressed)
 TO inset these into the ipynb, need to change the location to your dataset.
 

 
